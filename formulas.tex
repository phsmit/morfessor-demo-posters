 \documentclass[final]{beamer} % beamer 3.10: do NOT use option hyperref={pdfpagelabels=false} !
  %\documentclass[final,hyperref={pdfpagelabels=false}]{beamer} % beamer 3.07: get rid of beamer warnings
  \mode<presentation> {  %% check http://www-i6.informatik.rwth-aachen.de/~dreuw/latexbeamerposter.php for examples
    \usetheme{Aalto}    %% you should define your own theme e.g. for big headlines using your own logos 
  }
  \usepackage[english]{babel}
  \usepackage[latin1]{inputenc}
  \usepackage{amsmath,amsthm, amssymb, latexsym}
  %\usepackage{times}\usefonttheme{professionalfonts}  % times is obsolete
  \usefonttheme[onlymath]{serif}
  \boldmath
  \usepackage[orientation=landscape,size=a2,scale=0.8,grid]{beamerposter}                       % e.g. for DIN-A0 poster
  %\usepackage[orientation=portrait,size=a1,scale=1.4,grid,debug]{beamerposter}                  % e.g. for DIN-A1 poster, with optional grid and debug output
  %\usepackage[size=custom,width=200,height=120,scale=2,debug]{beamerposter}                     % e.g. for custom size poster
  %\usepackage[orientation=portrait,size=a0,scale=1.0,printer=rwth-glossy-uv.df]{beamerposter}   % e.g. for DIN-A0 poster with rwth-glossy-uv printer check

\usepackage{natbib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathpazo}


\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}



  % ...
  %
  \title{Morfessor 2.0: Toolkit for Statistical Morphological Segmentation -- Model}
  \author{Peter Smit, Sami Virpioja, Stig-Arne Gr\"onroos and Mikko Kurimo}
  \institute[Aalto University]{Aalto University}
  \date{April 28th, 2014}

\newlength{\columnheight}
\setlength{\columnheight}{105cm}
\usetikzlibrary{positioning}
\usetikzlibrary{fit}
\usetikzlibrary{calc}
\usetikzlibrary{shapes.geometric}

\definecolor{myblue}{HTML}{86B0FF}
\definecolor{mybluedark}{HTML}{3660CF}
\definecolor{myorange}{HTML}{FFCB94}
\definecolor{mygreen}{HTML}{A2C594}
\definecolor{mygrey}{HTML}{737379}

\definecolor{webgreen}{rgb}{0,.5,0}
\definecolor{webbrown}{rgb}{.6,0,0}
\definecolor{Maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{RoyalBlue}{cmyk}{1, 0.50, 0, 0}

\usepackage{textcomp}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\ind}{I}
\DeclareMathOperator*{\sign}{sign}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\mbf}[1]{\boldsymbol{#1}}
\newcommand{\seq}[1]{\boldsymbol{#1}}
\newcommand{\dom}[1]{\mathcal{#1}}
\newcommand{\txt}[1]{\textrm{#1}}

\newcommand{\len}[1]{\lvert#1\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}

\newcommand{\cost}{L}
\newcommand{\model}{\mathcal{M}}
\newcommand{\params}{\boldsymbol{\theta}}
\newcommand{\hparams}{\boldsymbol{\hat{\theta}}}
\newcommand{\tparams}{\boldsymbol{\tilde{\theta}}}
\newcommand{\paramspace}{\Theta}
\newcommand{\data}{\seq{D}}
\newcommand{\hatdata}{\seq{\hat{D}}}
\newcommand{\traindata}{\seq{D_{\txt{train}}}}
\newcommand{\testdata}{\seq{D_{\txt{test}}}}
\newcommand{\grammar}{\mathcal{G}}
\newcommand{\lexicon}{\mathcal{L}}
\newcommand{\corpus}{\textrm{corpus}}

%\newcommand{\word}{\phi^{-1}}
\newcommand{\detoken}{\phi^{-1}}
\newcommand{\token}{\phi}
\newcommand{\tokenset}{\Phi}
\newcommand{\bound}{\#}

\newcommand{\vb}{\,|\,}

\newcommand{\X}{\mat{X}}
\newcommand{\Y}{\mat{Y}}
\newcommand{\Z}{\mat{Z}}

  \begin{document}
  \begin{frame}{Morfessor 2.0: Toolkit for Statistical Morphological Segmentation -- Model} 
\begin{columns}

\begin{column}{.3\textwidth}
%      \begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
% \begin{block}{Morfessor model}
%              \begin{itemize}
%              \item Probabilistic
%		\item Machine learning
%\item Unsupervised or semi-supervised
%		\item Morphological segmentation
%              \end{itemize}              
%Full description in  \citep{virpioja2012thesis, virpioja2013morfessor}    
%        \end{block}
%            
%
%	\end{beamercolorbox}

      \begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
 \begin{block}{Probabilistic Model definition}
              \begin{itemize}
\item Full description in  \citep{virpioja2012thesis, virpioja2013morfessor} \\
              \item Generative model\\
$p(\underset{analyses}{A}, \underset{words}{W} \vb \underset{parameters}{\params})$\\
The model generates pairs of words and analysis (the segmentation of a word into morphs)
		\item Tokenization function\\
 $\seq{a} = \token(\seq{w}; \params)$
		\item Cost derivation\\
\begin{align*}\params_{\txt{MAP}}%&= \argmax_{\params} p(\params \vb \data)\\
&= \argmax_{\params} p(\params) p(\underset{data}{\data} \vb \params)\\
\cost(\params, \data) &= -\log \underset{prior}{p(\params)} - \log \underset{data\,likelihood}{p(\data \vb \params)}
\end{align*}
The data ($\data$) is a list of (non-segmented) words to learn the model from in unsupervised manner.
              \end{itemize}              
            \end{block}
            
	\end{beamercolorbox}

      \begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
 \begin{block}{Data Likelihood}
\begin{align*}
  \log p(\data \vb \params)
  & = \sum_{j=1}^{N} \log p(W=\seq{w}_j \vb \params) \nonumber \\
  & = \sum_{j=1}^{N} \log \sum_{\seq{a} \in \tokenset(\seq{w}_j)}
  p(A=\seq{a} \vb \params),
\end{align*}

Morfessor Baseline assumes independence of words. Also, only valid tokenisations of need to be considered.

Morfessor selects only one tokenisation (analysis) for each word at a time, by introducing a hidden variable $\Y$. 
\begin{align*}
  \log p(\data \vb \params, \Y)
  & = \sum_{j=1}^{N} \log 
  p(y_j \vb \params) 
  \\&= \sum_{j=1}^{N} \log 
  p(\underset{selected\,analysis}{m_{j1}, \ldots, m_{j\len{\seq{y}_j}}, \bound_w} \vb \params) 
\label{eq:morphylikelihood}
\end{align*}

            \end{block}
	\end{beamercolorbox}




\end{column}

\begin{column}{.4\textwidth}


      \begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
 \begin{block}{Prior}
\begin{align*}
p(\params) & = p(\underset{lexicon}{\lexicon})p(\underset{grammar}{\grammar})
\end{align*}

Morfessor Baseline has no grammar, only a prior over the lexicon

\begin{align*}
p(\params)  =& p(\underset{\#morphs}{\mu}) \times\\& p(\txt{properties}(m_1), \ldots, \\&\txt{properties}(m_{\mu})) \times  \\ &\underset{\#morph\,permutations}{\mu!}.
\end{align*}

\begin{equation*}
  p(\underset{morph}{\seq{\sigma}_i}) = \underset{length\,prior}{p(L=\len{\seq{\sigma}_i})}
  \prod_{j=1}^{\len{\seq{\sigma}_i}} \underset{character\,distribution}{p(C=\seq{\sigma}_{ij})}
\end{equation*}

            \end{block}
	\end{beamercolorbox}

\vfill

      \begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
 \begin{block}{Algorithm}
\begin{algorithmic}
  \Function{LocalBatchTrain}{$\data$, $\epsilon$}
  \State $\params, \Y \gets \textsc{InitModel}(\data_W)$ 
  \State $L_{\txt{old}} \gets \infty$
  \State $L_{\txt{new}} \gets L(\data_W, \params, \Y)$
  \While{$L_{\txt{new}} < L_{\txt{old}} - \epsilon$}
  \State $\seq{J} \gets \textsc{RandomPermutation}(1, \ldots, N)$
  \For{$j \in \seq{J}$}
  \State $\params, \Y \gets \textsc{LocalSearch}(\seq{w}_j, \data, \params, \Y)$
  %\State $\Y[\seq{w}_j] \gets \textsc{LocalSearch}(\seq{w}_j, \data, \params, \Y)$
  %\State $\params \gets \argmin_{\params}  L(\data, \params, \Y)$
  \EndFor
  \State $L_{\txt{old}} \gets L_{\txt{new}}$
  \State $L_{\txt{new}} \gets L(\data, \params, \Y)$
  \EndWhile
  \State \Return $\params$, $\Y$
  \EndFunction
\end{algorithmic}


%            \end{block}
%	\end{beamercolorbox}
%
%\vfill
%
%      \begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
% \begin{block}{Algorithm}
\vspace{-1cm}
%\tikzstyle{every node}=[font=\footnotesize]
\tikzstyle{morph} = [rectangle, draw, thin, text centered, node distance=0.4cm, minimum size=0.6cm, rounded corners=.8ex, fill=myorange!60]
\tikzstyle{morph2} = [rectangle, draw, thin, dashed, text centered, node distance=0.5cm, minimum size=0.6cm, rounded corners=.8ex, fill=myorange!60]
\tikzstyle{hidden} = [node distance=0.5cm, minimum size=0.6cm]
\resizebox{0.9\textwidth}{!}{
\begin{tikzpicture}[scale=1.0,thick,>=latex,->,font=\fontsize{7}{4}\selectfont]
\node[hidden] (top1) at (0,0) {};
\node[hidden] (top2) [right=3.0cm of top1] {};
\node[morph2] (unmatched) [below=of top1] {un+matched (1, 1)};
\node[morph2] (matchboxes) [below=of top2] {match+boxes (1, 1)};
\node[morph2] (matched) [below=of unmatched] {match+ed (1, 2)};
\node[morph] (un) [left=of matched] {\textbf{ un (0, 1)}};
\node[morph2] (boxes) [below=of matchboxes] {box+es (2, 3)};
\node[morph] (match) [below=of matched] { \textbf{match (5, 8)}};
\node[morph] (ed) [right=of match] {\textbf{ ed (0, 2)}};
\node[morph] (box) [below=of boxes] {\textbf {box (7, 10)}};
\node[morph] (es) [right=of box] {\textbf{ es (0, 3)}};
\path (unmatched) edge node {} (un); 
\path (unmatched) edge node {} (matched); 
\path (matchboxes) edge node {} (match); 
\path (matchboxes) edge node {} (boxes); 
\path (boxes) edge node {} (box); 
\path (boxes) edge node {} (es); 
\path (matched) edge node {} (match); 
\path (matched) edge node {} (ed); 
\path (top1) edge node {} (unmatched); 
\path (top2) edge node {} (matchboxes); 
\end{tikzpicture}
}
 \textbf{unmatched}, \textbf{matchboxes},
  \textbf{matched}, \textbf{boxes}, \textbf{match}, and
  \textbf{box}
            \end{block}
	\end{beamercolorbox}









%  \begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
% \begin{block}{Workflow}
%\tikzstyle{sbox} = [draw, thin, fill=myblue!50,
%  text width=14em, text centered, node distance=2.0cm]
%\tikzstyle{sabox} = [draw, thin, dashed, fill=myblue!50,
%  text width=14em, text centered, node distance=2.0cm]
%\tikzstyle{dbox} = [draw, rounded corners=.8ex, text width=8em, text centered, node distance=2.0cm]
%\tikzstyle{dabox} = [draw, dashed, rounded corners=.8ex, text width=8em, text centered, node distance=2.0cm]
%\tikzstyle{nobox} = [text width=8em, text centered, node distance=2.0cm]
%\resizebox{\textwidth}{!}{
%\begin{tikzpicture}[yscale=2,thick,>=latex]
%\node[dbox] (traindata) at (0,0) {Training data compounds $\data$};
%\node[sbox] (training) [right=of traindata] {(1) Model training: \\ $\argmin_{\params} L(\data, \data_{A}, \params)$};
%\node[dabox] (annottraindata) at (0,2) {Annotated training data $\data_{W \to A}$};
%\node[sbox] (tokenization) [below=of training] {(2) Decoding:\\ $A = \token(W; \params)$};
%\node[dbox] (testdata) [left=of tokenization] {Test data compounds};
%\node[dbox] (testconstr) [below=of tokenization] {Test data constructions};
%\draw[->] (traindata) to node [above] {$W$} (training);
%\draw[->,dashed] (annottraindata.east) to node [above] {$W, A$} (training);
%\draw[->] (training) to node [right] {$\params$} (tokenization);
%\draw[->] (testdata) to node [above] {$W$} (tokenization);
%\draw[->] (tokenization) to node [right] {$A$} (testconstr);
%
%%\node[sabox] (dtokenization) [above=of training] {Decoding: $A = \token(\hat{W}; \params)$};
%%\node[dabox,text width=10em] (develdata) [above=of dtokenization] {Annotated development data $\hatdata_{W \to A}$};
%%\node[sabox,text width=6em] (eval) [right=of dtokenization] {Evaluation: $S = d(A, \hat{A})$};
%%\draw[->,dashed] (develdata) to node [right] {$\hat{W}$} (dtokenization);
%%\draw[->,dashed] (develdata) to node [above] {$\hat{A}$} (eval);
%%\draw[->,dashed] (training) to node [right] {$\params$} (dtokenization);
%%\draw[->,dashed] (dtokenization) to node [above] {$A$} (eval);
%%\draw[->,dashed] (eval.south) to node [above] {$S$} (training.east);
%\end{tikzpicture}}            \end{block}
%            
%	\end{beamercolorbox}
%
%
%
%
%





\end{column}
\begin{column}{.3\textwidth}




      \begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
 \begin{block}{Likelihood weighting and Semi-supervised training }
Likelihood weighting with $\alpha$ \citep{kohonen2010semisupervised,virpioja2011nodalida}
\begin{align*}
  \cost(\params, \data) =& - \log p(\params) \\&- \alpha \log p(\data
  \vb \params)
\end{align*}

$\alpha$ can be determined in different ways, e.g using a development set, or some explicit knowledge like average morph length. Higher $\alpha$ reduces segmentation, lower $\alpha$ increases segmentation.

Semi-supervised \citep{kohonen2010sigmorphon}
\begin{align*}
  \cost(\params, \data) = & - \log p(\params) \\&- \alpha \log p(\data
  \vb \params) \\&- \beta \log p(\underset{annotated\,data}{\data_{A}} \vb \params).
\end{align*}

For semi-supervised learning another term is added to the cost, the likelihood of a set of annotations coming from the model. Also here a weight $\beta$ is introduced to control the effect.

            \end{block}
	\end{beamercolorbox}


  \begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
 \begin{block}{References}
\footnotesize
             \bibliographystyle{apalike} %plain
\bibliography{report}
           
            \end{block}
            
	\end{beamercolorbox}




\end{column}

\end{columns}
 \end{frame}



  \end{document}
  